{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "# for text classification\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## araBERT Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"bert-base-arabert\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    text = arabert_prep.preprocess(text)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits.softmax(dim=1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري\"\n",
    "probs = classify(text)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# ai_df = pd.read_csv('../Tweets/AIArabicTweets.csv')\n",
    "# ai_df.columns = ['text', 'label']\n",
    "# # set all the labels to 0\n",
    "# ai_df['label'] = 0\n",
    "# ai_df = ai_df.drop(0)\n",
    "\n",
    "# human_df = pd.read_csv('../Tweets/HumanArabicTweets.csv')\n",
    "# human_df.columns = ['text', 'label']\n",
    "# # set all the labels to 1\n",
    "# human_df['label'] = 1\n",
    "\n",
    "# # merge the two dataframes\n",
    "# df = pd.concat([ai_df, human_df], ignore_index=True)\n",
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "# # drop nan values\n",
    "# df = df.dropna()\n",
    "\n",
    "# # save the dataframe to a csv file\n",
    "# df.to_csv('ArabicTweets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"ArabicTweets.csv\", split=\"train\", encoding=\"windows-1256\")\n",
    "# find number of examples for each label\n",
    "human = sum([1 for label in dataset[\"label\"] if label == 1])\n",
    "ai = sum([1 for label in dataset[\"label\"] if label == 0])\n",
    "\n",
    "print(\"human: \", human,\"ai: \", ai)\n",
    "# balance the dataset\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "human_dataset = dataset.filter(lambda example: example[\"label\"] == 1)\n",
    "ai_dataset = dataset.filter(lambda example: example[\"label\"] == 0)\n",
    "filtered_human_dataset = human_dataset.select(range(ai))\n",
    "\n",
    "balanced_dataset = concatenate_datasets([filtered_human_dataset, ai_dataset])\n",
    "dataset = balanced_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = sum([1 for label in balanced_dataset[\"label\"] if label == 1])\n",
    "ai = sum([1 for label in balanced_dataset[\"label\"] if label == 0])\n",
    "\n",
    "print(\"human: \", human,\"ai: \", ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check average length of the tweets\n",
    "import numpy as np\n",
    "lengths = [len(tweet) for tweet in dataset[\"train\"][\"text\"]]\n",
    "print(np.mean(lengths))\n",
    "print(np.max(lengths))\n",
    "print(np.min(lengths))\n",
    "# find number of tweets having \n",
    "num = sum([1 for tweet in dataset[\"train\"][\"text\"] if len(tweet) > 100])\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying araBERT Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_preprocess(examples):\n",
    "    return {\"text\":arabert_prep.preprocess(examples[\"text\"]), \"label\":examples[\"label\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(arabic_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"Training Sentence Lengths: \")\n",
    "plt.hist([ len(tokenizer.tokenize(sentence)) for sentence in dataset[\"train\"][\"text\"]],bins=range(0,128,2))\n",
    "plt.show()\n",
    "\n",
    "print(\"Testing Sentence Lengths: \")\n",
    "plt.hist([ len(tokenizer.tokenize(sentence)) for sentence in dataset[\"test\"][\"text\"]],bins=range(0,128,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    result = tokenizer(examples[\"text\"],truncation=True,   \n",
    "                       max_length=128, return_overflowing_tokens=True)\n",
    "\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "    # return tokenizer(arabic_prep, truncation=True, max_length=512, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of labels\n",
    "num_labels = len(set(dataset[\"train\"][\"label\"]))\n",
    "print(num_labels)\n",
    "\n",
    "# get label names\n",
    "label_names = [\"AI\", \"Human\"]\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {label: i for i, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.hidden_dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-2\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import evaluate\n",
    "from torch.optim import AdamW\n",
    "exp = \"5\"\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./logs/araBERT-base_exp\"+exp)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "metric = evaluate.load(\"glue\", \"mrpc\", device=device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# lr_scheduler\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    # training epoch\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        scores = outputs.logits\n",
    "        probs = scores.softmax(dim=1)\n",
    "\n",
    "        predictions = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(train_loss=loss.item())\n",
    "\n",
    "    train_out = metric.compute()\n",
    "    train_accuracy = train_out[\"accuracy\"]\n",
    "    train_f1 = train_out[\"f1\"]\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} - Train loss: {train_loss:.4f}, accuracy: {train_accuracy:.4f}, f1: {train_f1:.4f}\")\n",
    "    \n",
    "    # Write to tensorboard\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"train/accuracy\", train_accuracy, epoch)\n",
    "    writer.add_scalar(\"train/f1\", train_f1, epoch)\n",
    "\n",
    "    # evaluating epoch\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        scores = outputs.logits\n",
    "        probs = scores.softmax(dim=1)\n",
    "        predictions = torch.argmax(probs, dim=-1)\n",
    "        eval_loss += loss.item()\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    eval_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_out = metric.compute()\n",
    "    eval_accuracy = eval_out[\"accuracy\"]\n",
    "    eval_f1 = eval_out[\"f1\"]\n",
    "    # eval_accuracy, eval_f1, recall, precision, eval_loss =  evaluate_test_set(model, tokenizer, dataset[\"test\"], batch_size=256)\n",
    "    print(f\"Epoch {epoch} - Eval loss: {eval_loss:.4f}, accuracy: {eval_accuracy:.4f}, f1: {eval_f1:.4f}\")\n",
    "\n",
    "    # Write to tensorboard\n",
    "    writer.add_scalar(\"eval/loss\", eval_loss, epoch)\n",
    "    writer.add_scalar(\"eval/accuracy\", eval_accuracy, epoch)\n",
    "    writer.add_scalar(\"eval/f1\", eval_f1, epoch)\n",
    "\n",
    "    # progress_bar.update(len(train_dataloader))\n",
    "    # add text to the progress bar\n",
    "    progress_bar.set_postfix(epochs = epoch,\n",
    "        train_loss=train_loss, eval_loss=eval_loss, train_acc=train_accuracy, eval_acc=eval_accuracy\n",
    "    )\n",
    "\n",
    "    model.save_pretrained(\"./trained/araBERT-base_exp\"+exp+\"/checkpoint-\"+str((epoch+1)*len(train_dataloader)))\n",
    "    tokenizer.save_pretrained(\"./trained/araBERT-base_exp\"+exp+\"/checkpoint-\"+str((epoch+1)*len(train_dataloader)))\n",
    "\n",
    "\n",
    "# close the tensorboard writer\n",
    "writer.close()\n",
    "\n",
    "# save the model\n",
    "# model.save_pretrained(\"./trained/araBERT-base_exp\"+exp)\n",
    "# tokenizer.save_pretrained(\"./trained/araBERT-base_exp\"+exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"sst2\", device=\"cuda\")\n",
    "def calculate_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    metric_computed = metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {\"accuracy\": metric_computed[\"accuracy\"], \"f1\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "exp = \"2\"\n",
    "step = int(len(tokenized_dataset[\"train\"])/(batch_size))\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained/araBERT-base\"+\"_exp\"+exp,\n",
    "    learning_rate=5e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=2*batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    save_total_limit=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=step,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=step,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs/araBERT-base\"+\"_exp\"+exp,\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = step\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=calculate_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"ArabicTweets.csv\", split=\"train[:5%]\", encoding=\"windows-1256\")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_path = \"trained/araBERT-base_exp4/checkpoint-21600\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "text = dataset[\"train\"][0][\"text\"]\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-31 13:11:36,698 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-arabert\"\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "arabic_prep = ArabertPreprocessor(model_name=model_name)\n",
    "def arabic_preprocess(examples):\n",
    "    return {\"text\":arabert_prep.preprocess(examples[\"text\"]), \"label\":examples[\"label\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69bbd0aec4d4f53ac4ea327fd194c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf5a343fd544b228a2ae6e8bf471c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(arabic_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "metric = evaluate.load(\"glue\", \"sst2\", device=device)\n",
    "# import precision_recall_fscore_support\n",
    "def evaluate_test_set(model, tokenizer, test_dataset, batch_size=32, pbar = False):\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else :\n",
    "        device = \"cpu\"\n",
    "    print(f\"Device: {device}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    f1 = 0\n",
    "    recall = 0\n",
    "    precision = 0\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    if pbar:\n",
    "        progress = tqdm(enumerate(test_dataloader), total=len(test_dataloader))\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        scores = outputs.logits\n",
    "        probs = scores.softmax(dim=1)\n",
    "        predicted_class_ids = probs.argmax(dim=1).tolist()\n",
    "\n",
    "        predictions.extend(predicted_class_ids)\n",
    "        batch_labels = batch[\"label\"]\n",
    "        labels.extend([x.item() for x in batch_labels])\n",
    "        unique_labels = np.unique(predicted_class_ids)\n",
    "\n",
    "        # batch_accuracy = sum([1 if label == prediction else 0 for label, prediction in zip(batch_labels, predicted_class_ids)])\n",
    "        # batch_accuracy = batch_accuracy / len(batch_labels)\n",
    "\n",
    "        batch_metric = metric.compute(predictions=predicted_class_ids, references=batch_labels)\n",
    "        batch_acc = batch_metric[\"accuracy\"]\n",
    "        batch_f1 = f1_score(batch_labels, predicted_class_ids, average='weighted', labels=unique_labels)\n",
    "        batch_recall = recall_score(batch_labels, predicted_class_ids, average='weighted', labels=unique_labels)\n",
    "        batch_precision = precision_score(batch_labels, predicted_class_ids, average='weighted', labels=unique_labels)\n",
    "        # print(batch_accuracy, batch_f1, batch_recall, batch_precision)\n",
    "    \n",
    "        acc += batch_metric[\"accuracy\"]\n",
    "        f1 += batch_f1\n",
    "        recall += batch_recall\n",
    "        precision += batch_precision\n",
    "\n",
    "        if pbar:\n",
    "            progress.update(1)\n",
    "            progress.set_description(f\"Batch {idx+1}/{len(test_dataloader)}: {batch_acc*100:.2f}%\")\n",
    "        # print(f\"Batch {idx+1}/{len(test_dataloader)}: {batch_accuracy}\")\n",
    "\n",
    "\n",
    "    acc = acc / len(test_dataloader)\n",
    "    f1 = f1 / len(test_dataloader)\n",
    "    recall = recall / len(test_dataloader)\n",
    "    precision = precision / len(test_dataloader)\n",
    "    return acc, f1, recall, precision, predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* مع مهروس ال+ ميراميه ال+ خضراء خارجي +ا ل+ علاج عقص ال+ حشر +ات\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for example in dataset[\"test\"]:\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5274a4f7f3c4832ab8b8a6cac37b5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy, f1, recall, precision, predictions, labels =  evaluate_test_set(model, tokenizer, dataset[\"test\"], batch_size=128, pbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.75%, F1: 97.29%, Recall: 100.00%, Precision: 94.75%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy*100:.2f}%, F1: {f1*100:.2f}%, Recall: {recall*100:.2f}%, Precision: {precision*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "len(predictions), len(labels)\n",
    "print(set(predictions))\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.00      0.00      0.00        94\n",
      "       Human       0.94      1.00      0.97      1579\n",
      "\n",
      "    accuracy                           0.94      1673\n",
      "   macro avg       0.47      0.50      0.49      1673\n",
      "weighted avg       0.89      0.94      0.92      1673\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahlarious/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/shahlarious/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/shahlarious/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(labels, predictions, labels=[0, 1], target_names=[\"AI\", \"Human\"])\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
